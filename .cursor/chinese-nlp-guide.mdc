---
description: ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†æŒ‡å¯¼
globs: 
alwaysApply: false
---
# ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†æŒ‡å¯¼

## ä¸­æ–‡æ–‡æœ¬å¤„ç†ç‰¹æ®Šæ€§

### åˆ†è¯æŒ‘æˆ˜
- ä¸­æ–‡æ²¡æœ‰å¤©ç„¶çš„è¯è¾¹ç•Œåˆ†éš”ç¬¦
- è¯è¯­åˆ†æ­§é—®é¢˜ï¼ˆå¦‚ï¼š"ç ”ç©¶ç”Ÿå‘½çš„èµ·æº" å¯åˆ†ä¸ºä¸åŒç»„åˆï¼‰
- æ–°è¯ã€ç½‘ç»œç”¨è¯­è¯†åˆ«å›°éš¾
- ä¸“ä¸šæœ¯è¯­å’Œå“ç‰Œåç§°å¤„ç†

### æƒ…æ„Ÿåˆ†æç‰¹ç‚¹
- ä¸­æ–‡æƒ…æ„Ÿè¡¨è¾¾æ›´åŠ å«è“„å’Œé—´æ¥
- è¯­åºå’Œè¯­æ°”åŠ©è¯å¯¹æƒ…æ„Ÿçš„å½±å“
- äºŒæ¬¡å…ƒæ–‡åŒ–ç›¸å…³çš„ç‰¹æ®Šæƒ…æ„Ÿè¯æ±‡
- ç½‘ç»œç”¨è¯­å’Œè¡¨æƒ…ç¬¦å·çš„æƒ…æ„Ÿå€¾å‘

## æ ¸å¿ƒNLPå·¥å…·é…ç½®

### jiebaåˆ†è¯å™¨è®¾ç½®
```python
import jieba
import jieba.posseg as pseg

# æ·»åŠ è‡ªå®šä¹‰è¯å…¸ï¼ˆAIç¡¬ä»¶ç›¸å…³ï¼‰
jieba.load_userdict("ai_hardware_dict.txt")

# è‡ªå®šä¹‰è¯å…¸ç¤ºä¾‹å†…å®¹ï¼š
# AIé™ªä¼´ 3 n
# è™šæ‹Ÿå¥³å‹ 3 n  
# æ™ºèƒ½éŸ³ç®± 3 n
# å® ç‰©æœºå™¨äºº 3 n
# äºŒæ¬¡å…ƒ 2 n

def custom_tokenize(text):
    """è‡ªå®šä¹‰åˆ†è¯å‡½æ•°"""
    # ç²¾ç¡®æ¨¡å¼åˆ†è¯
    words = jieba.lcut(text, cut_all=False)
    # è¿‡æ»¤å•å­—å’Œåœç”¨è¯
    words = [w for w in words if len(w) > 1 and w not in stopwords]
    return words
```

### åœç”¨è¯å¤„ç†
```python
# åŠ è½½ä¸­æ–‡åœç”¨è¯è¡¨
with open('chinese_stopwords.txt', 'r', encoding='utf-8') as f:
    stopwords = set(f.read().strip().split('\n'))

# æ·»åŠ é¡¹ç›®ç‰¹å®šåœç”¨è¯
custom_stopwords = {'çœŸçš„', 'å°±æ˜¯', 'æ„Ÿè§‰', 'è§‰å¾—', 'å¯èƒ½', 'åº”è¯¥'}
stopwords.update(custom_stopwords)
```

### æƒ…æ„Ÿåˆ†æå·¥å…·
```python
from snownlp import SnowNLP

def analyze_sentiment(text):
    """ä¸­æ–‡æƒ…æ„Ÿåˆ†æ"""
    s = SnowNLP(text)
    sentiment_score = s.sentiments
    
    if sentiment_score > 0.6:
        return 'æ­£é¢'
    elif sentiment_score < 0.4:
        return 'è´Ÿé¢'
    else:
        return 'ä¸­æ€§'

# ç»“åˆæƒ…æ„Ÿè¯å…¸çš„æ–¹æ³•
def sentiment_with_dict(text, pos_dict, neg_dict):
    """åŸºäºæƒ…æ„Ÿè¯å…¸çš„åˆ†æ"""
    words = custom_tokenize(text)
    pos_count = sum(1 for word in words if word in pos_dict)
    neg_count = sum(1 for word in words if word in neg_dict)
    
    if pos_count > neg_count:
        return 'æ­£é¢'
    elif neg_count > pos_count:
        return 'è´Ÿé¢'
    else:
        return 'ä¸­æ€§'
```

## AIç¡¬ä»¶é¢†åŸŸè¯å…¸æ„å»º

### äº§å“ç±»å‹è¯å…¸
```python
product_type_dict = {
    'AIéŸ³ç®±': ['æ™ºèƒ½éŸ³ç®±', 'è¯­éŸ³åŠ©æ‰‹', 'å°çˆ±åŒå­¦', 'å¤©çŒ«ç²¾çµ', 'å°åº¦'],
    'é™ªä¼´æœºå™¨äºº': ['å® ç‰©æœºå™¨äºº', 'é™ªä¼´æœºå™¨äºº', 'AIç‹—', 'AIçŒ«', 'æ™ºèƒ½å® ç‰©'],
    'è™šæ‹Ÿä¼´ä¾£': ['è™šæ‹Ÿå¥³å‹', 'è™šæ‹Ÿç”·å‹', 'AIå¥³å‹', 'AIä¼´ä¾£'],
    'äºŒæ¬¡å…ƒæ‰‹åŠ': ['æ™ºèƒ½æ‰‹åŠ', 'ä¼šè¯´è¯çš„æ‰‹åŠ', 'è¯­éŸ³æ‰‹åŠ', 'åŠ¨æ¼«æœºå™¨äºº'],
    'AIç©å…·': ['æ™ºèƒ½ç©å…·', 'AIç©å¶', 'ä¼šèŠå¤©çš„ç©å…·']
}
```

### æƒ…æ„Ÿè¯å…¸æ‰©å±•
```python
# æ­£é¢æƒ…æ„Ÿè¯ï¼ˆAIç¡¬ä»¶ç›¸å…³ï¼‰
positive_words = [
    'å¯çˆ±', 'èŒ', 'æ²»æ„ˆ', 'æ¸©æš–', 'é™ªä¼´', 'æœ‰è¶£', 'æ™ºèƒ½', 
    'å¥½ç”¨', 'å®ç”¨', 'å€¼å¾—', 'æ¨è', 'æ»¡æ„', 'å–œæ¬¢', 'çˆ±äº†'
]

# è´Ÿé¢æƒ…æ„Ÿè¯
negative_words = [
    'æ— èŠ', 'é¸¡è‚‹', 'å¤±æœ›', 'åæ‚”', 'å‘', 'åƒåœ¾', 'éš¾ç”¨',
    'å¡é¡¿', 'ååº”æ…¢', 'å£°éŸ³éš¾å¬', 'ä¸å€¼', 'æµªè´¹é’±'
]

# äºŒæ¬¡å…ƒç‰¹è‰²è¯æ±‡
anime_words = [
    'è€å©†', 'è€å…¬', 'å¥³ç¥', 'ç”·ç¥', 'çº¸ç‰‡äºº', 'ä¸‰æ¬¡å…ƒ', 
    'waifu', 'å«ç»™', 'å¨¶', 'CP', 'æ”»å—', 'èŒå±æ€§'
]
```

## æ–‡æœ¬é¢„å¤„ç†ç®¡é“

### æ¸…æ´—æµç¨‹
```python
import re

def clean_chinese_text(text):
    """ä¸­æ–‡æ–‡æœ¬æ¸…æ´—"""
    # å»é™¤URL
    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
    
    # å»é™¤@ç”¨æˆ·å
    text = re.sub(r'@[\w\u4e00-\u9fff]+', '', text)
    
    # å»é™¤å¤šä½™çš„ç©ºæ ¼å’Œæ¢è¡Œ
    text = re.sub(r'\s+', ' ', text).strip()
    
    # ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—å’Œå¸¸ç”¨æ ‡ç‚¹
    text = re.sub(r'[^\u4e00-\u9fff\u0030-\u0039\u0041-\u005a\u0061-\u007a\uff01-\uff5e\u3000-\u303f]', '', text)
    
    return text

def extract_keywords(text, top_k=10):
    """å…³é”®è¯æå–"""
    import jieba.analyse
    
    # TF-IDFå…³é”®è¯æå–
    keywords_tfidf = jieba.analyse.extract_tags(text, topK=top_k, withWeight=True)
    
    # TextRankå…³é”®è¯æå–
    keywords_textrank = jieba.analyse.textrank(text, topK=top_k, withWeight=True)
    
    return keywords_tfidf, keywords_textrank
```

## ç‰¹æ®Šå¤„ç†ç­–ç•¥

### ç½‘ç»œç”¨è¯­å¤„ç†
```python
# ç½‘ç»œç”¨è¯­æ˜ å°„è¡¨
internet_slang_dict = {
    'yyds': 'æ°¸è¿œçš„ç¥',
    'ç»ç»å­': 'ç»äº†',
    'èŠ­æ¯”Q': 'å®Œè›‹äº†', 
    '6': 'å‰å®³',
    '666': 'å¾ˆå‰å®³',
    'awsl': 'å¯çˆ±æ­»äº†',
    'xswl': 'ç¬‘æ­»æˆ‘äº†'
}

def normalize_internet_slang(text):
    """ç½‘ç»œç”¨è¯­æ ‡å‡†åŒ–"""
    for slang, normal in internet_slang_dict.items():
        text = text.replace(slang, normal)
    return text
```

### è¡¨æƒ…ç¬¦å·å¤„ç†
```python
import emoji

def process_emojis(text):
    """è¡¨æƒ…ç¬¦å·å¤„ç†"""
    # æå–è¡¨æƒ…ç¬¦å·
    emoji_list = emoji.emoji_list(text)
    
    # å°†è¡¨æƒ…è½¬æ¢ä¸ºæ–‡å­—æè¿°
    text_with_emoji_desc = emoji.demojize(text, language='zh')
    
    # æˆ–è€…ç›´æ¥ç§»é™¤è¡¨æƒ…
    text_no_emoji = emoji.replace_emoji(text, replace='')
    
    return text_with_emoji_desc, emoji_list

# æƒ…æ„Ÿè¡¨æƒ…æ˜ å°„
emoji_sentiment = {
    'ğŸ˜Š': 'positive', 'ğŸ˜‚': 'positive', 'â¤ï¸': 'positive',
    'ğŸ˜­': 'negative', 'ğŸ˜¡': 'negative', 'ğŸ’”': 'negative',
    'ğŸ˜': 'neutral', 'ğŸ¤”': 'neutral'
}
```

## æ•°æ®è´¨é‡æ§åˆ¶

### æ–‡æœ¬è´¨é‡è¯„ä¼°
```python
def assess_text_quality(text):
    """æ–‡æœ¬è´¨é‡è¯„ä¼°"""
    quality_score = 0
    
    # é•¿åº¦åˆç†æ€§ï¼ˆ10-500å­—ç¬¦ï¼‰
    if 10 <= len(text) <= 500:
        quality_score += 1
    
    # ä¸­æ–‡å­—ç¬¦å æ¯”
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    if chinese_chars / len(text) > 0.5:
        quality_score += 1
    
    # éé‡å¤å­—ç¬¦æ¯”ä¾‹
    unique_chars = len(set(text))
    if unique_chars / len(text) > 0.3:
        quality_score += 1
    
    return quality_score >= 2
```

### å¼‚å¸¸æ–‡æœ¬è¯†åˆ«
```python
def detect_spam_text(text):
    """åƒåœ¾æ–‡æœ¬è¯†åˆ«"""
    spam_indicators = [
        'åŠ å¾®ä¿¡', 'ä¼˜æƒ åˆ¸', 'é™æ—¶ç‰¹ä»·', 'ç‚¹å‡»é“¾æ¥',
        'ä»£è´­', 'å›¢è´­', 'åˆ·å•', 'å¹¿å‘Š'
    ]
    
    spam_count = sum(1 for indicator in spam_indicators if indicator in text)
    return spam_count >= 2
```
